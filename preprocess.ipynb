{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Hf-HVAd6IAY_","outputId":"050b39d9-0246-4807-b05e-836587075235"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package words to /home/srijan2022/nltk_data...\n","[nltk_data]   Package words is already up-to-date!\n"]}],"source":["import nltk\n","from nltk.corpus import words\n","\n","nltk.download('words')\n","word_dictionary = list(set(words.words()))\n","\n","\n","for alphabet in \"bcdefghjklmnopqrstuvwxyz\":\n","\tword_dictionary.remove(alphabet)\n","\n","def split_hashtag_to_words_all_possibilities(hashtag):\n","\tall_possibilities = []\n","\t\n","\tsplit_posibility = [hashtag[:i] in word_dictionary for i in reversed(range(len(hashtag)+1))]\n","\tpossible_split_positions = [i for i, x in enumerate(split_posibility) if x == True]\n","\t\n","\tfor split_pos in possible_split_positions:\n","\t\tsplit_words = []\n","\t\tword_1, word_2 = hashtag[:len(hashtag)-split_pos], hashtag[len(hashtag)-split_pos:]\n","\t\t\n","\t\tif word_2 in word_dictionary:\n","\t\t\tsplit_words.append(word_1)\n","\t\t\tsplit_words.append(word_2)\n","\t\t\tall_possibilities.append(split_words)\n","\n","\t\t\tanother_round = split_hashtag_to_words_all_possibilities(word_2)\n","\t\t\t\t\n","\t\t\tif len(another_round) > 0:\n","\t\t\t\tall_possibilities = all_possibilities + [[a1] + a2 for a1, a2, in zip([word_1]*len(another_round), another_round)]\n","\t\telse:\n","\t\t\tanother_round = split_hashtag_to_words_all_possibilities(word_2)\n","\t\t\t\n","\t\t\tif len(another_round) > 0:\n","\t\t\t\tall_possibilities = all_possibilities + [[a1] + a2 for a1, a2, in zip([word_1]*len(another_round), another_round)]\n","\t\n","\treturn all_possibilities"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bsMffWAkIAZE","outputId":"9cdd018a-4678-4d7a-8317-ec123d87e17a"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     /home/srijan2022/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["import re\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","nltk.download('stopwords')\n","\n","stop_words = set(stopwords.words('english'))\n","\n","def preprocess_without_stopwords(df , COL=\"tweet\"):\n","    newdf= df.copy()\n","\n","    for ind in df.index:\n","\n","        a = re.sub(r'\\B@[\\S]+\\b',' ', df[COL][ind])            # remove usernames  \n","        b = re.sub(r'(\\bhttp:\\/\\/\\S+\\.\\S+\\b)|(\\bwww\\.\\S+\\.\\S+\\b)|(\\bhttps:\\/\\/\\S+\\.\\S+\\b)',' ' , a)  # remove url\n","        c = re.sub(r'&\\S+;',' ', b)        # godlike optimsitaion &#number; replaced with space\n","        d = re.sub(r'\\.+','.', c)                   # replace multiple \".\"\n","        e = re.sub(r'!+','!', d)                    # replace multiple \"!\"\n","        f = re.sub(r'\\?+','?',e)                    # replace multiple \"?\"\n","        g = re.sub(r'\\\"+',' ',f)                    # replace multiple \" *\"* \"\n","        h = re.sub(r':',' ',g)                      # replace multiple \":\"\n","        i = re.sub(r'\\bRT\\b',' ',h)                 # remove retweet keyword  \"RT\"\n","\n","        # can remove all special characters and numbers except ' . - \n","\n","        j = re.sub(r'\\s+',' ',i)                    # remove multiple whitespaces\n","\n","        temp = j.lower().split()\n","        temp2=[]\n","\n","        #print(temp)\n","\n","        for i in range(len(temp)) :\n","            if(temp[i][0]=='#'):\n","                if temp[i][1:] in word_dictionary :                  # made separate case to hadle a bug #repsect not being detecetd \n","                    temp2.append(temp[i][1:]) \n","                else :\n","                        if split_hashtag_to_words_all_possibilities(temp[i][1:]) != [] :\n","                            temp2.extend(split_hashtag_to_words_all_possibilities(temp[i][1:])[0])\n","            else :\n","                    temp2.append(temp[i])     \n","\n","        # print(temp2)       \n","\n","        filtered_sentence=[]\n","        for w in temp2:\n","            if w not in stop_words:\n","                filtered_sentence.append(w)\n","        \n","\n","\n","        sentence = ' '.join([str(elem) for elem in filtered_sentence])\n","\n","\n","        newdf[COL][ind] = newdf[COL][ind].replace(newdf[COL][ind],sentence)\n","    return newdf\n","\n","# preprocess(test_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CNv2NrqxIAZG","outputId":"07ec9e24-0cca-454f-a240-488ab07eb391"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     /home/srijan2022/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["import re\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","nltk.download('stopwords')\n","\n","stop_words = set(stopwords.words('english'))\n","\n","def preprocess_with_stopwords(df , COL=\"tweet\"):\n","    newdf= df.copy()\n","\n","    for ind in df.index:\n","\n","        a = re.sub(r'\\B@[\\S]+\\b',' ', df[COL][ind])            # remove usernames  \n","        b = re.sub(r'(\\bhttp:\\/\\/\\S+\\.\\S+\\b)|(\\bwww\\.\\S+\\.\\S+\\b)|(\\bhttps:\\/\\/\\S+\\.\\S+\\b)',' ' , a)  # remove url\n","        c = re.sub(r'&\\S+;',' ', b)        # godlike optimsitaion &#number; replaced with space\n","        d = re.sub(r'\\.+','.', c)                   # replace multiple \".\"\n","        e = re.sub(r'!+','!', d)                    # replace multiple \"!\"\n","        f = re.sub(r'\\?+','?',e)                    # replace multiple \"?\"\n","        g = re.sub(r'\\\"+',' ',f)                    # replace multiple \" *\"* \"\n","        h = re.sub(r':',' ',g)                      # replace multiple \":\"\n","        i = re.sub(r'\\bRT\\b',' ',h)                 # remove retweet keyword  \"RT\"\n","\n","        # can remove all special characters and numbers except ' . - \n","\n","        j = re.sub(r'\\s+',' ',i)                    # remove multiple whitespaces\n","\n","        temp = j.lower().split()\n","        temp2=[]\n","\n","        #print(temp)\n","\n","        for i in range(len(temp)) :\n","            if(temp[i][0]=='#'):\n","                if temp[i][1:] in word_dictionary :                  # made separate case to hadle a bug #repsect not being detecetd \n","                    temp2.append(temp[i][1:]) \n","                else :\n","                        if split_hashtag_to_words_all_possibilities(temp[i][1:]) != [] :\n","                            temp2.extend(split_hashtag_to_words_all_possibilities(temp[i][1:])[0])\n","            else :\n","                    temp2.append(temp[i])     \n","\n","        # print(temp2)       \n","\n","        # filtered_sentence=[]\n","        # for w in temp2:\n","        #     if w not in stop_words:\n","        #         filtered_sentence.append(w)\n","        \n","\n","\n","        sentence = ' '.join([str(elem) for elem in temp2])\n","\n","\n","        newdf[COL][ind] = newdf[COL][ind].replace(newdf[COL][ind],sentence)\n","    return newdf\n","\n","# preprocess(test_df)"]}],"metadata":{"kernelspec":{"display_name":"Python 3.6.9 ('venv': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"40c8aee14bfcd74aa3931897f14b05b05ebb9b88f6cef370cd02f0c54cc83820"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}